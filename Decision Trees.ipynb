{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 -  Decision Trees\n",
    "The following notebook takes you through implementing the decision tree algorithm. It involves data manipulation/visualisation, hyperparameter selection, recursion, and building a prediction model. We will use a binary classification problem: Breast cancer diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking and Submission\n",
    "\n",
    "This lab exercise is marked, and contributes 10% to your final grade. For this lab exercise there are 6 questions for which you are expected to enter your own code, for 15 marks overall. Every place you have to add code is indicated by\n",
    "\n",
    "`# **************************************************************** n marks`\n",
    "\n",
    "with instructions above the code block.\n",
    "\n",
    "Please submit your completed workbook on [https://you.cs.bath.ac.uk] before 2021-11-5 20:00 GMT. The workbook you submit must be an `.ipynb` file, which is saved into the directory you're running Jupyter; alternatively you can download it from the menu above using `File -> Download As -> Notebook (.ipynb)`. Remember to save your work regularly (Save and checkpoint in the File menu, the icon of a floppy disk, or Ctrl-S); the version you submit should have all code blocks showing the results (if any) of execution below them. The auto markerr will provide feedback and you can submit as many times as you want. You do not need to include the dataset when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets as ds\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "The first step of any machine learning problem is to load the data. In this tutorial you don't have to download any dataset since we are using a built-in dataset provided by the scikit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = ds.load_breast_cancer()\n",
    "\n",
    "x = data_all.data\n",
    "y = data_all.target\n",
    "\n",
    "y_names = data_all.target_names \n",
    "\n",
    "feature_names = data_all.feature_names\n",
    "\n",
    "#print(data_all.keys())\n",
    "#for i in x:\n",
    "#    print(i)\n",
    "#print(feature_names)\n",
    "#print(y_names)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Wisconsin (Diagnostic) Database\n",
    "A description of the dataset used is provided here.\n",
    "\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 569\n",
    "\n",
    "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "    :Attribute Information:\n",
    "        - radius (mean of distances from center to points on the perimeter)\n",
    "        - texture (standard deviation of gray-scale values)\n",
    "        - perimeter\n",
    "        - area\n",
    "        - smoothness (local variation in radius lengths)\n",
    "        - compactness (perimeter^2 / area - 1.0)\n",
    "        - concavity (severity of concave portions of the contour)\n",
    "        - concave points (number of concave portions of the contour)\n",
    "        - symmetry \n",
    "        - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "        largest values) of these features were computed for each image,\n",
    "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "        13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "        - target class:\n",
    "                - WDBC-Malignant\n",
    "                - WDBC-Benign\n",
    "\n",
    "    :Summary Statistics:\n",
    "\n",
    "    ===================================== ====== ======\n",
    "                                           Min    Max\n",
    "    ===================================== ====== ======\n",
    "    radius (mean):                        6.981  28.11\n",
    "    texture (mean):                       9.71   39.28\n",
    "    perimeter (mean):                     43.79  188.5\n",
    "    area (mean):                          143.5  2501.0\n",
    "    smoothness (mean):                    0.053  0.163\n",
    "    compactness (mean):                   0.019  0.345\n",
    "    concavity (mean):                     0.0    0.427\n",
    "    concave points (mean):                0.0    0.201\n",
    "    symmetry (mean):                      0.106  0.304\n",
    "    fractal dimension (mean):             0.05   0.097\n",
    "    radius (standard error):              0.112  2.873\n",
    "    texture (standard error):             0.36   4.885\n",
    "    perimeter (standard error):           0.757  21.98\n",
    "    area (standard error):                6.802  542.2\n",
    "    smoothness (standard error):          0.002  0.031\n",
    "    compactness (standard error):         0.002  0.135\n",
    "    concavity (standard error):           0.0    0.396\n",
    "    concave points (standard error):      0.0    0.053\n",
    "    symmetry (standard error):            0.008  0.079\n",
    "    fractal dimension (standard error):   0.001  0.03\n",
    "    radius (worst):                       7.93   36.04\n",
    "    texture (worst):                      12.02  49.54\n",
    "    perimeter (worst):                    50.41  251.2\n",
    "    area (worst):                         185.2  4254.0\n",
    "    smoothness (worst):                   0.071  0.223\n",
    "    compactness (worst):                  0.027  1.058\n",
    "    concavity (worst):                    0.0    1.252\n",
    "    concave points (worst):               0.0    0.291\n",
    "    symmetry (worst):                     0.156  0.664\n",
    "    fractal dimension (worst):            0.055  0.208\n",
    "    ===================================== ====== ======\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "\n",
    "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "\n",
    "    :Donor: Nick Street\n",
    "\n",
    "    :Date: November, 1995\n",
    "\n",
    "This is a copy of the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset from https://goo.gl/U2Uwz2\n",
    "\n",
    "Features are computed from a digitized image of a fine needle\n",
    "aspirate (FNA) of a breast mass. They describe\n",
    "characteristics of the cell nuclei present in the image.\n",
    "\n",
    "Separating plane described above was obtained using\n",
    "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "Construction Via Linear Programming.\" Proceedings of the 4th\n",
    "Midwest Artificial Intelligence and Cognitive Science Society,\n",
    "pp. 97-101, 1992], a classification method which uses linear\n",
    "programming to construct a decision tree.  Relevant features\n",
    "were selected using an exhaustive search in the space of 1-4\n",
    "features and 1-3 separating planes.\n",
    "\n",
    "The actual linear program used to obtain the separating plane\n",
    "in the 3-dimensional space is that described in:\n",
    "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
    "Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server:\n",
    "\n",
    "```\n",
    "ftp ftp.cs.wisc.edu\n",
    "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "```\n",
    "\n",
    "### References\n",
    "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
    "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
    "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
    "     San Jose, CA, 1993.\n",
    "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
    "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
    "     July-August 1995.\n",
    "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
    "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
    "     163-171.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare/Split data\n",
    "We provide the data preparation part. The bellow code block splits the data and the targets into training and test sets; 60% for training, 40% for test. This repartition is of course arbitrary, different percentages could have been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 341\n",
      "Test set size: 228\n"
     ]
    }
   ],
   "source": [
    "split = int(x.shape[0] * 0.6)\n",
    "\n",
    "x_train = x[:split,:]\n",
    "y_train = y[:split]\n",
    "\n",
    "x_test = x[split:,:]\n",
    "y_test = y[split:]\n",
    "\n",
    "print('Training set size:', x_train.shape[0])\n",
    "print('Test set size:', x_test.shape[0])\n",
    "\n",
    "#print(x_train)\n",
    "#print('--')\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "Since our data has a feature dimensionality of 30, it is difficult for us to visualise it. We visualize data by using a dimensionality reduction technique called Principal Component Analysis (PCA). \n",
    "\n",
    "Given an array in `R^None` (a matrix of size `n X d` with real entries) with `n` and `d` being the number of data points and the feature dimensionality, respectively, PCA will output an array in `R^None`, with `m<d`. \n",
    "\n",
    "PCA will be covered in future lectures. But for now, you can consider it as a way to reduce the dimensionality of our feature space. \n",
    "\n",
    "In order to be able to visualise the data on a 2D plot, we choose `m=2` (`m=3` is also a possibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Looking at Data\n",
    "\n",
    "Complete the code block below to plot the reduced data obtained using PCA (`x_reduced` contains the result of applying PCA). Use different colours and markers to distinguish between positive and negative samples.\n",
    "\n",
    "It is interesting to see how the results havea  different visualisation without the scaling part. \n",
    "\n",
    "The results should look similar to the plot below (please note that this is a plot of another dataset):\n",
    "\n",
    "<img src=\"pca_example_graph.png\">\n",
    "\n",
    "Hint: You will need to google the documentation for the `scatter()` and `legend()` methods of `matplotlib`.\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_scaled = preprocessing.scale(x[:,:-1]) # We remove the indexing and make sure all the features are in N(0,1)\n",
    "#print(x_scaled)\n",
    "x_reduced = pca.fit_transform(x_scaled)\n",
    "#x_reduced = pca.fit_transform(x[:,0:-1]) # Uncomment this to see the result without scaling\n",
    "#print(x_reduced[:,:])\n",
    "\n",
    "malignent_x = x_reduced[:,0][y==1]\n",
    "malignent_y = x_reduced[:,1][y==1]\n",
    "\n",
    "benign_x = x_reduced[:,0][y==0]\n",
    "benign_y = x_reduced[:,1][y==0]\n",
    "# **************************************************************** 1 mark\n",
    "plt.scatter(malignent_x, malignent_y, color='orange', marker='*', label='Benign')\n",
    "plt.scatter(benign_x, benign_y, color='blue', marker='+', label='Malignent')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculating Entropy\n",
    "\n",
    "Complete the function `calculate_entropy(y)` in the code block bellow. The input is a column vector of target class values, and the output is its entropy (as described in the decision tree lecture).\n",
    "\n",
    "`y` is a length `n` vector where `n` is the number of data points. It contains the class/category of each data point as an integer, i.e. `0` for the first class, `1` for the second class.\n",
    "The return is a scalar.\n",
    "\n",
    "Hints:\n",
    " * You may want to google the documentation for `numpy.unique()`,  paying particular attention to the `return_counts` keyword.\n",
    " * Beware `log(0)`.\n",
    " * Be careful about type - you may need to use `.astype(float)` to avoid integer division.\n",
    "\n",
    "__(4 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of 'y' is: 0.9526\n"
     ]
    }
   ],
   "source": [
    "def calculate_entropy(y):\n",
    "    # **************************************************************** 4 marks\n",
    "    num = np.unique(y, return_counts = True)[1]\n",
    "    \n",
    "    if len(y) == 0 or all(i == y[0] for i in y):\n",
    "        return 0\n",
    "    \n",
    "    else:      \n",
    "        mal = num[0]\n",
    "        ben = num[1]\n",
    "        p_mal = float(mal / len(y))\n",
    "        p_ben = float(ben / len(y))\n",
    "    \n",
    "        entropy = - ((p_mal * np.log2(p_mal)) + (p_ben * np.log2(p_ben))).astype(float)  \n",
    "    return entropy\n",
    "    \n",
    "print(\"The entropy of 'y' is: {:.4f}\".format(calculate_entropy(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Doing the Splits\n",
    "\n",
    "Use the function `calculate_entropy()` to complete the function `find_split(x, y)`.\n",
    "\n",
    "`find_split(x, y)` takes as input:\n",
    " * The data matrix of features, `x` with shape `(n,d)`. `n` is the number of data points and `d` is the feature dimensionality. \n",
    " * `y`, a column vector of size `n` containing the target value for each data point in `x`.\n",
    "\n",
    "`find_split(x, y)` outputs 'best_split' which is a dictionary (see the last part of the below code) with the following keys and their corresponding values:\n",
    "\n",
    " * `'feature'`: An integer indexing the attribute/feature chosen to split upon.\n",
    " * `'split'`: The value/threshold of this feature to split at.\n",
    " * `'infogain'`: A scalar representing the amount of information gained by splitting this way.\n",
    " * `'left_indices'`: Indices of the exemplars that satisfy `x[feature_index]<=split`.\n",
    " * `'right_indices'`: Opposite set of indices to `left_indices`.\n",
    "\n",
    "__(5 marks)__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_split(x, y):\n",
    "    \"\"\"Given a dataset and its target values, this finds the optimal combination\n",
    "    of feature and split point that gives the maximum information gain.\"\"\"\n",
    "    \n",
    "    # Need the starting entropy so we can measure improvement...\n",
    "    start_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Best thus far, initialised to a dud that will be replaced immediately...\n",
    "    best = {'infogain' : -np.inf}\n",
    "    \n",
    "    # Loop every possible split of every dimension...\n",
    "    for i in range(x.shape[1]):\n",
    "        for split in np.unique(x[:,i]):\n",
    "            left_indices = np.where(x[:,i] <= split)[0]\n",
    "            right_indices = np.where(x[:,i] > split)[0]\n",
    "\n",
    "            n_left = y[left_indices]\n",
    "            n_right = y[right_indices]\n",
    "            #print(n_left)\n",
    "           \n",
    "            sum_nleft = (len(n_left)) * calculate_entropy(n_left)\n",
    "            sum_nright = (len(n_right)) * calculate_entropy(n_right)\n",
    "            #avg_ent = (sum_nleft + sum_nright)/len(y)\n",
    "\n",
    "            #Note infogain is start entropy - (average entropy of new splits)\n",
    "            infogain = start_entropy - ((sum_nleft + sum_nright) / len(y))\n",
    "             \n",
    "            \n",
    "            # **************************************************************** 5 marks\n",
    "            \n",
    "            if infogain > best['infogain']:\n",
    "                best = {'feature' : i,\n",
    "                        'split' : split,\n",
    "                        'infogain' : infogain, \n",
    "                        'left_indices' : left_indices,\n",
    "                        'right_indices' : right_indices}\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(x, y):\n",
    "\n",
    "    start_entropy = calculate_entropy(y)\n",
    "    \n",
    "    best = {'infogain' : -np.inf}\n",
    "    \n",
    "    for i in range(x.shape[1]):\n",
    "        for split in np.unique(x[:,i]):\n",
    "            \n",
    "            left_indices = []\n",
    "            right_indices = []\n",
    "            \n",
    "            for index in range(x.shape[0]):\n",
    "                if x[index, i]>split:                      \n",
    "                    right_indices.append(index)            \n",
    "                else:\n",
    "                    left_indices.append(index)             \n",
    "            \n",
    "            nl = float(len(left_indices))                  \n",
    "            nr = float(len(right_indices))\n",
    "            n = nl + nr                                    \n",
    "            \n",
    "            infogain = start_entropy - (nl/n)*calculate_entropy(y[left_indices]) - (nr/n)*calculate_entropy(y[right_indices]) \n",
    "            if infogain > best['infogain']:\n",
    "                best = {'feature' : i,\n",
    "                        'split' : split,\n",
    "                        'infogain' : infogain, \n",
    "                        'left_indices' : left_indices,\n",
    "                        'right_indices' : right_indices}            \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': 22,\n",
       " 'split': 105.9,\n",
       " 'infogain': 0.561986885126551,\n",
       " 'left_indices': [3,\n",
       "  5,\n",
       "  9,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  37,\n",
       "  38,\n",
       "  40,\n",
       "  41,\n",
       "  44,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  55,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  63,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  71,\n",
       "  74,\n",
       "  76,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  84,\n",
       "  88,\n",
       "  90,\n",
       "  92,\n",
       "  93,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  106,\n",
       "  107,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  120,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  128,\n",
       "  130,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  139,\n",
       "  140,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  163,\n",
       "  165,\n",
       "  166,\n",
       "  169,\n",
       "  170,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  178,\n",
       "  179,\n",
       "  183,\n",
       "  185,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  195,\n",
       "  200,\n",
       "  204,\n",
       "  206,\n",
       "  208,\n",
       "  211,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  224,\n",
       "  226,\n",
       "  228,\n",
       "  229,\n",
       "  231,\n",
       "  232,\n",
       "  234,\n",
       "  235,\n",
       "  238,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  251,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  273,\n",
       "  275,\n",
       "  276,\n",
       "  278,\n",
       "  279,\n",
       "  281,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  301,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  322,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  336,\n",
       "  338,\n",
       "  341,\n",
       "  342,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  364,\n",
       "  367,\n",
       "  371,\n",
       "  374,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  390,\n",
       "  391,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  407,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  415,\n",
       "  416,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  431,\n",
       "  434,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  442,\n",
       "  443,\n",
       "  445,\n",
       "  447,\n",
       "  450,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  466,\n",
       "  467,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  477,\n",
       "  478,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  485,\n",
       "  488,\n",
       "  490,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  502,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  510,\n",
       "  511,\n",
       "  515,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  534,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  568],\n",
       " 'right_indices': [0,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  39,\n",
       "  42,\n",
       "  43,\n",
       "  45,\n",
       "  53,\n",
       "  54,\n",
       "  56,\n",
       "  57,\n",
       "  62,\n",
       "  64,\n",
       "  65,\n",
       "  70,\n",
       "  72,\n",
       "  73,\n",
       "  75,\n",
       "  77,\n",
       "  78,\n",
       "  82,\n",
       "  83,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  89,\n",
       "  91,\n",
       "  94,\n",
       "  95,\n",
       "  99,\n",
       "  100,\n",
       "  105,\n",
       "  108,\n",
       "  112,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  121,\n",
       "  122,\n",
       "  126,\n",
       "  127,\n",
       "  129,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  138,\n",
       "  141,\n",
       "  147,\n",
       "  148,\n",
       "  156,\n",
       "  157,\n",
       "  161,\n",
       "  162,\n",
       "  164,\n",
       "  167,\n",
       "  168,\n",
       "  171,\n",
       "  172,\n",
       "  177,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  184,\n",
       "  186,\n",
       "  190,\n",
       "  194,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  205,\n",
       "  207,\n",
       "  209,\n",
       "  210,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  218,\n",
       "  219,\n",
       "  223,\n",
       "  225,\n",
       "  227,\n",
       "  230,\n",
       "  233,\n",
       "  236,\n",
       "  237,\n",
       "  239,\n",
       "  244,\n",
       "  250,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  272,\n",
       "  274,\n",
       "  277,\n",
       "  280,\n",
       "  282,\n",
       "  283,\n",
       "  291,\n",
       "  300,\n",
       "  302,\n",
       "  317,\n",
       "  321,\n",
       "  323,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  335,\n",
       "  337,\n",
       "  339,\n",
       "  340,\n",
       "  343,\n",
       "  347,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  363,\n",
       "  365,\n",
       "  366,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  372,\n",
       "  373,\n",
       "  375,\n",
       "  389,\n",
       "  392,\n",
       "  393,\n",
       "  400,\n",
       "  406,\n",
       "  408,\n",
       "  413,\n",
       "  414,\n",
       "  417,\n",
       "  421,\n",
       "  430,\n",
       "  432,\n",
       "  433,\n",
       "  435,\n",
       "  441,\n",
       "  444,\n",
       "  446,\n",
       "  448,\n",
       "  449,\n",
       "  451,\n",
       "  460,\n",
       "  461,\n",
       "  465,\n",
       "  468,\n",
       "  472,\n",
       "  476,\n",
       "  479,\n",
       "  484,\n",
       "  486,\n",
       "  487,\n",
       "  489,\n",
       "  491,\n",
       "  492,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  503,\n",
       "  508,\n",
       "  509,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  516,\n",
       "  517,\n",
       "  521,\n",
       "  533,\n",
       "  535,\n",
       "  541,\n",
       "  542,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_split(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function `find_split()` allows us to find the optimal feature and the best value to split the data into two chunks (on its own it is the _decision stump_ algorithm). Applying this to the original data set splits it into two new data sets. We can then repeat this on both of the new data sets to get four data sets, and so on. This recursion builds a decision tree. It needs a stopping condition, to prevent it dividing the data forever, here we will use two:\n",
    " * Maximum depth: The tree is limited to be no deeper than a provided limit.\n",
    " * Perfection: If a node contains only one class then it does not make sense to split it further.\n",
    "\n",
    "We provide the function `build_tree(x, y, max_depth)` below to construct a tree. The inputs are: \n",
    "\n",
    " * The data matrix of features, `x` in `R^None`. `n` is the number of data points and `d` is the feature dimensionality. \n",
    " * `y`, a column vector of size `n` containing the target value for each data point in `x`.\n",
    " * The maximum depth of the tree, `max_depth`.\n",
    "\n",
    "The output of this function is a dictionary. If it has generated a leaf node then the keys are:\n",
    " * `'leaf' : True`\n",
    " * `'class'` : The index of the class to assign to exemplars that land here.\n",
    "\n",
    "If it has generated a split node then the keys are:\n",
    " * `'leaf' : False`\n",
    " * `'feature'`: The feature to apply the `split` to.\n",
    " * `'split'`: The split to test the exemplars `feature` with.\n",
    " * `'infogain'`: The information gain of this split.\n",
    " * `'left'` : The left subtree, for exemplars where `x[feature_index]<=split`\n",
    " * `'right'` : The right subtree, for exemplars where `x[feature_index]>split`\n",
    "\n",
    "Note how this structure is compatable with the one returned by `find_split()` above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(x, y, max_depth = np.inf):\n",
    "    # Check if either of the stopping conditions have been reached. If so generate a leaf node...\n",
    "    if max_depth==1 or (y==y[0]).all():\n",
    "        # Generate a leaf node...\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return {'leaf' : True, 'class' : classes[np.argmax(counts)]}\n",
    "    \n",
    "    else:\n",
    "        move = find_split(x, y)\n",
    "        \n",
    "        left = build_tree(x[move['left_indices'],:], y[move['left_indices']], max_depth - 1)\n",
    "        right = build_tree(x[move['right_indices'],:], y[move['right_indices']], max_depth - 1)\n",
    "        \n",
    "        return {'leaf' : False,\n",
    "                'feature' : move['feature'],\n",
    "                'split' : move['split'],\n",
    "                'infogain' : move['infogain'],\n",
    "                'left' : left,\n",
    "                'right' : right}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After building the tree we should be able to predict the class of a sample. We do that by propagating the sample through the tree, i.e. we check all the splitting conditions until the sample falls in a leaf node, in which case the class of the leaf node is attributed to the sample.\n",
    "\n",
    "We provide the recursive function `predict_one(tree, sample)` that takes as input the constructed tree, a sample in `R^d` and recursively propagates it through the branches of our tree. The output of this function is the class predicted for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(tree, sample):\n",
    "    \"\"\"Does the prediction for a single data point\"\"\"\n",
    "    if tree['leaf']:\n",
    "        return tree['class']\n",
    "    \n",
    "    else:\n",
    "        if sample[tree['feature']] <= tree['split']:\n",
    "            return predict_one(tree['left'], sample)\n",
    "        else:\n",
    "            return predict_one(tree['right'], sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further generalize the prediction function above to the case where we have a data matrix `R^None` representing many data points. the function `predict(tree, samples)` bellow takes as input the constructed tree and a data array then returns an array containing the predictions for all the samples in our input data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, samples):\n",
    "    \"\"\"Predicts class for every entry of a data matrix.\"\"\"\n",
    "    ret = np.empty(samples.shape[0], dtype=int)\n",
    "    ret.fill(-1)\n",
    "    indices = np.arange(samples.shape[0])\n",
    "    \n",
    "    def tranverse(node, indices):\n",
    "        nonlocal samples\n",
    "        nonlocal ret\n",
    "        \n",
    "        if node['leaf']:\n",
    "            ret[indices] = node['class']\n",
    "        \n",
    "        else:\n",
    "            going_left = samples[indices, node['feature']] <= node['split']\n",
    "            left_indices = indices[going_left]\n",
    "            right_indices = indices[np.logical_not(going_left)]\n",
    "            \n",
    "            if left_indices.shape[0] > 0:\n",
    "                tranverse(node['left'], left_indices)\n",
    "                \n",
    "            if right_indices.shape[0] > 0:\n",
    "                tranverse(node['right'], right_indices)\n",
    "    \n",
    "    tranverse(tree, indices)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Accuracy:\n",
    "Use the functions defined above to build a tree and report both its training and test accuracy.\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 89.91%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(x_train, y_train, x_test, y_test, max_depth):\n",
    "    \n",
    "    # **************************************************************** 2 marks\n",
    "    tree = build_tree(x_train, y_train, max_depth)\n",
    "    train_pred = predict(tree, x_train)\n",
    "    test_pred = predict(tree, x_test)\n",
    "    train_acc = len(np.where(train_pred == y_train)[0]) / len(train_pred)\n",
    "    test_acc = len(np.where(test_pred == y_test)[0]) / len(test_pred)\n",
    "    return train_acc, test_acc\n",
    "\n",
    "\n",
    "train_acc, test_acc = evaluate(x_train, y_train, x_test, y_test, np.inf)\n",
    "print('Train Accuracy: {:.2f}%'.format(train_acc*100))\n",
    "print('Test Accuracy: {:.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimal Tree Depth\n",
    "\n",
    "Find the best `max_depth` parameter plus its corresponding training and test accuracies. A good range to test is `range(2,6)`.\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is 4, and the corresponding training and test accuracies are 97.36% and 91.67% respectively.\n"
     ]
    }
   ],
   "source": [
    "def find_best_max_depth(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # **************************************************************** 2 marks\n",
    "    best_max_depth = 0\n",
    "    best_train_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    for i in range(2,6):\n",
    "        train_acc, test_acc = evaluate(x_train, y_train, x_test, y_test, i)\n",
    "        if test_acc > best_test_acc:\n",
    "            best_train_acc = train_acc\n",
    "            best_test_acc = test_acc\n",
    "            best_max_depth = i\n",
    "    \n",
    "    return best_max_depth, best_train_acc, best_test_acc\n",
    "\n",
    "\n",
    "best_max_depth, best_train_acc, best_test_acc = find_best_max_depth(x_train, y_train, x_test, y_test)\n",
    "print('The best max_depth is {}, and the corresponding training and test accuracies are {:.2f}% and {:.2f}% respectively.'.format(best_max_depth,best_train_acc*100,best_test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Looking at Trees\n",
    "\n",
    "Write a recursive function that prints out a tree, and use it to print the <b>best</b> tree learned.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "    [x22 <=0.296]\n",
    "\n",
    "        [x27 <=-0.058]\n",
    "\n",
    "          [x13 <=0.187]\n",
    "\n",
    "           [x21 <=1.246]\n",
    "                predict 1\n",
    "                \n",
    "           [x21 >1.246]\n",
    "                predict 0\n",
    "                \n",
    "          [x13 >0.187]\n",
    "\n",
    "           [x0 <=0.160]\n",
    "                predict 1\n",
    "                \n",
    "           [x0 >0.160]\n",
    "                predict 0\n",
    "                \n",
    "        [x27 >-0.058]\n",
    "\n",
    "          [x27 <=0.690]\n",
    "\n",
    "           [x21 <=0.263]\n",
    "            predict 1\n",
    "            \n",
    "           [x21 >0.263]\n",
    "            predict 0\n",
    "            \n",
    "          [x27 >0.690]\n",
    "            predict 0\n",
    "            \n",
    "    [x22 >0.296]\n",
    "\n",
    "    predict 0\n",
    "```\n",
    "\n",
    "The conditions with the same tree depth must be indented the same amount. This function should have as input the tree learned, and a scalar `indent` that is used to measure how far to indent at the current recursion level.\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [x22 <= 105.0]\n",
      "\n",
      "\t [x24 <= 0.1733]\n",
      "\n",
      "\t\t [x21 <= 23.31]\n",
      "\t\t\t predict 1\n",
      "\n",
      "\t\t [x21 > 23.31]\n",
      "\t\t\t predict 1\n",
      "\n",
      "\t [x24 > 0.1733]\n",
      "\t\t predict 0\n",
      "\n",
      " [x22 > 105.0]\n",
      "\n",
      "\t [x22 <= 114.3]\n",
      "\n",
      "\t\t [x1 <= 19.65]\n",
      "\t\t\t predict 1\n",
      "\n",
      "\t\t [x1 > 19.65]\n",
      "\t\t\t predict 0\n",
      "\n",
      "\t [x22 > 114.3]\n",
      "\n",
      "\t\t [x7 <= 0.02771]\n",
      "\t\t\t predict 1\n",
      "\n",
      "\t\t [x7 > 0.02771]\n",
      "\t\t\t predict 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tree(tree, indent = 0):\n",
    "    tab = '\\t' * indent\n",
    "    \n",
    "    # **************************************************************** 1 mark\n",
    "    if tree['leaf'] == True:\n",
    "        print(tab,'predict', tree['class'])\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        indent += 1\n",
    "        print()\n",
    "        print(tab,'[x{} <= {}]'.format(tree['feature'], tree['split']))\n",
    "        print_tree(tree['left'], indent)\n",
    "        print(tab, '[x{} > {}]'.format(tree['feature'], tree['split']))\n",
    "        print_tree(tree['right'], indent)\n",
    "        \n",
    "    '''\n",
    "    if tree['leaf']:\n",
    "        return tree['class']\n",
    "    \n",
    "    else:\n",
    "        if sample[tree['feature']] <= tree['split']:\n",
    "            return predict_one(tree['left'], sample)\n",
    "        else:\n",
    "            return predict_one(tree['right'], sample)\n",
    "    '''\n",
    "\n",
    "tree = build_tree(x_train, y_train, best_max_depth)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
